{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Klaudio-Peqini/Solving-ODEs-using-PINNs/blob/main/Solving_ODEs_PINNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGrgw2YZdTrS"
      },
      "source": [
        "# **Solving ODEs with Physically Informed Neural Networks (PINNs)**\n",
        "\n",
        "Klaudio Peqini\n",
        "\n",
        "Department of Physics, Faculty of Natural Sciences, University of Tirana, blvd. Zogu I, No. 25, Tirane, Albania"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: 1st order ODE (exponential decay)\n",
        "\n",
        "\n",
        "$y'(x)+y(x)=0 \\hspace{0.3cm} with \\hspace{0.3cm} x_{min} < x < x_{max}$,\n",
        "            \n",
        "with initial condition (effectively a boundary condition) $y(0)=1$.\n",
        "\n",
        "*The analytical solution (to be compared with):  $y(x)=\\exp(-x)$*\n"
      ],
      "metadata": {
        "id": "Ph46sxGMyNxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main libraries"
      ],
      "metadata": {
        "id": "ldbOHsWmP9n8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5zN_fiefMhh"
      },
      "outputs": [],
      "source": [
        "# Tensorflow Keras and rest of the packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input,Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k6aCnLShFq0"
      },
      "source": [
        "## Define and construct the PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbdSBpTnrmSV"
      },
      "outputs": [],
      "source": [
        "class ODE_1st(tf.keras.Model):\n",
        "    def train_step(self, data):\n",
        "        # Training points and the analytical (exact) solution at this points\n",
        "        x, y_exact = data\n",
        "        # Initial conditions for the PINN\n",
        "        x0=tf.constant([0.0], dtype=tf.float32)\n",
        "        y0_exact=tf.constant([1.0], dtype=tf.float32)\n",
        "        # Calculate the gradients and update weights and bias\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Calculate the gradients dy/dx\n",
        "            with tf.GradientTape() as tape2:\n",
        "              tape2.watch(x0)\n",
        "              tape2.watch(x)\n",
        "              y0_NN = self(x0, training=True)\n",
        "              y_NN  = self(x, training=True)\n",
        "            dy_dx_NN= tape2.gradient(y_NN,x)\n",
        "            #Loss= ODE+ boundary/initial conditions\n",
        "            loss=self.compiled_loss(dy_dx_NN, -y_NN)\\\n",
        "                +self.compiled_loss(y0_NN,y0_exact)\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "        self.compiled_metrics.update_state(y_exact, y_NN)\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUc4dJ4DhKn8"
      },
      "source": [
        "## Run the PINN and tune it\n",
        "\n",
        "Here you are invited to \"play around\" with the $hyperparameters$ that characterize dhe PINN. Try several combinations and look for sets of values that yield a smaller Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoiLIsw3dTrT"
      },
      "outputs": [],
      "source": [
        "n_train = 20\n",
        "xmin = 0\n",
        "xmax = 4 # Here you can change xmin and xmax accordingly\n",
        "\n",
        "# Definition of the function domain\n",
        "x_train=np.linspace(xmin,xmax,n_train)\n",
        "\n",
        "# The real solution y(x) for training evaluation\n",
        "y_train=tf.exp(-x_train)\n",
        "\n",
        "# Input and output neurons (from the data)\n",
        "input_neurons  = 1\n",
        "output_neurons = 1\n",
        "\n",
        "# Hiperparameters\n",
        "epochs = 40\n",
        "\n",
        "# Definition of the the model\n",
        "activation='elu'\n",
        "input=Input(shape=(input_neurons,))\n",
        "x=Dense(50, activation=activation)(input)\n",
        "x=Dense(50, activation=activation)(x)\n",
        "x=Dense(50, activation=activation)(x)\n",
        "output = Dense(output_neurons,activation=None)(x)\n",
        "model=ODE_1st(input,output)\n",
        "\n",
        "# Definition of the metrics, optimizer and loss\n",
        "loss= tf.keras.losses.MeanSquaredError()\n",
        "metrics=tf.keras.metrics.MeanSquaredError()\n",
        "optimizer= Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "          optimizer=optimizer,\n",
        "          metrics=[metrics])\n",
        "model.summary()\n",
        "\n",
        "history=model.fit(x_train, y_train,batch_size=1,epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Evolution of losses during training"
      ],
      "metadata": {
        "id": "Yap3PTRi4Rhp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du_bMDsq7Hr5"
      },
      "outputs": [],
      "source": [
        "# summarize history for loss and metris\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.plot(history.history['loss'],color='magenta',\n",
        "         label='Total losses ($L_D + L_B$)')\n",
        "plt.plot(history.history['mean_squared_error'],color='cyan',label='MSE')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution and its derivatives (to compare the numerical solution with RK4 and PINN)"
      ],
      "metadata": {
        "id": "DSqGs-Xa4W5J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu5AeSzauTgN"
      },
      "outputs": [],
      "source": [
        "# Check the PINN at different points not included in the training set\n",
        "n = 500\n",
        "x=np.linspace(0,4,n)\n",
        "y_exact=tf.exp(-x)\n",
        "y_NN=model.predict(x)\n",
        "\n",
        "# The gradients (y'(x) and y''(x)) from the model\n",
        "x_tf = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "with tf.GradientTape(persistent=True) as t:\n",
        "  t.watch(x_tf)\n",
        "  with tf.GradientTape(persistent=True) as t2:\n",
        "        t2.watch(x_tf)\n",
        "        y = model(x_tf)\n",
        "  dy_dx_NN = t2.gradient(y, x_tf)\n",
        "d2y_dx2_NN = t.gradient(dy_dx_NN, x_tf)\n",
        "\n",
        "# Plot the results\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.plot(x, y_exact, color=\"black\",linestyle='solid',\n",
        "                     linewidth=2.5,label=\"$y(x)$ analytical\")\n",
        "plt.plot(x, y_NN, color=\"red\",linestyle='dashed',\n",
        "                     linewidth=2.5, label=\"$y_{NN}(x)$\")\n",
        "plt.plot(x, dy_dx_NN, color=\"blue\",linestyle='-.',\n",
        "                     linewidth=3.0, label=\"$y'_{NN}(x)$\")\n",
        "plt.plot(x, d2y_dx2_NN, color=\"green\", linestyle='dotted',\n",
        "                     linewidth=3.0, label=\"$y''_{NN}(x)$\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: $2^{nd}$ order ODE (harmonic oscillations)\n",
        "\n",
        "$y''(x)+y(x)=0 \\hspace{0.3cm} with \\hspace{0.3cm} x_{min} < x < x_{max}$,\n",
        "            \n",
        "with initial conditions (effectively a boundary conditions) $y(0)=1$ and $y'(0)=0$.\n",
        "\n",
        "*The analytical solution (to be compared with):  $y(x)=\\cos(x)$*\n"
      ],
      "metadata": {
        "id": "ViHpsrhFTRRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import relevant libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "glnZ2xHtUKq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ajzjdx-fUeh0"
      },
      "outputs": [],
      "source": [
        "# Tensorflow Keras and rest of the packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input,Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and construct the PINN"
      ],
      "metadata": {
        "id": "paOw07KIUr4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc2w3kUgNmiT"
      },
      "outputs": [],
      "source": [
        "class ODE_2nd(tf.keras.Model):\n",
        "    def train_step(self, data):\n",
        "        # Training points and the analytical\n",
        "        # (exact) solution at this points\n",
        "        x, y_exact = data\n",
        "        #Change initial conditions for the PINN\n",
        "        x0=tf.constant([0.0], dtype=tf.float32)\n",
        "        y0_exact=tf.constant([1.0], dtype=tf.float32)\n",
        "        dy_dx0_exact=tf.constant([0.0], dtype=tf.float32)\n",
        "        # Calculate the gradients and update weights and bias\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x)\n",
        "            tape.watch(y_exact)\n",
        "            tape.watch(x0)\n",
        "            tape.watch(y0_exact)\n",
        "            tape.watch(dy_dx0_exact)\n",
        "            # Calculate the gradients dy2/dx2, dy/dx\n",
        "            with tf.GradientTape() as tape0:\n",
        "                    tape0.watch(x0)\n",
        "                    y0_NN = self(x0,training=False)\n",
        "                    tape0.watch(y0_NN)\n",
        "            dy_dx0_NN = tape0.gradient(y0_NN, x0)\n",
        "            with tf.GradientTape() as tape1:\n",
        "                tape1.watch(x)\n",
        "                with tf.GradientTape() as tape2:\n",
        "                    tape2.watch(x)\n",
        "                    y_NN = self(x,training=False)\n",
        "                    tape2.watch(y_NN)\n",
        "                dy_dx_NN = tape2.gradient(y_NN, x)\n",
        "                tape1.watch(y_NN)\n",
        "                tape1.watch(dy_dx_NN)\n",
        "            d2y_dx2_NN = tape1.gradient(dy_dx_NN, x)\n",
        "            tape.watch(y_NN)\n",
        "            tape.watch(dy_dx_NN)\n",
        "            tape.watch(d2y_dx2_NN)\n",
        "\n",
        "            #Loss= ODE+ boundary/initial conditions\n",
        "            y0_exact=tf.reshape(y0_exact,shape=y_NN[0].shape)\n",
        "            dy_dx0_exact=tf.reshape(dy_dx0_exact,shape=dy_dx0_NN.shape)\n",
        "\n",
        "            loss= self.compiled_loss(y0_NN,y0_exact)\\\n",
        "                  +self.compiled_loss(d2y_dx2_NN,-y_NN)\\\n",
        "                  +self.compiled_loss(dy_dx0_NN,dy_dx0_exact)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "        self.compiled_metrics.update_state(y_exact, y_NN)\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the PINN and tune it\n",
        "\n",
        "Here you are invited to \"play around\" with the $hyperparameters$ that characterize dhe PINN. Try several combinations and look for sets of values that yield a smaller Loss Function"
      ],
      "metadata": {
        "id": "-rU768kGVOd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = 18\n",
        "xmin = 0.0\n",
        "xmax = 8.0\n",
        "\n",
        "# Definition of the function domain\n",
        "x_train=np.linspace(xmin,xmax,n_train)\n",
        "\n",
        "# The solution y(x) for training and validation datasets\n",
        "y_train=np.cos(x_train)\n",
        "\n",
        "# Input and output neurons (from the data)\n",
        "input_neurons  = 1\n",
        "output_neurons = 1\n",
        "\n",
        "# Hiperparameters\n",
        "epochs = 500\n",
        "\n",
        "# Definition of the the model\n",
        "initializer = tf.keras.initializers.GlorotUniform(seed=5)\n",
        "activation='tanh'\n",
        "input=Input(shape=(input_neurons,))\n",
        "x=Dense(50, activation=activation,\n",
        "            kernel_initializer=initializer)(input)\n",
        "x=Dense(50, activation=activation,\n",
        "            kernel_initializer=initializer)(x)\n",
        "x=Dense(50, activation=activation,\n",
        "            kernel_initializer=initializer)(x)\n",
        "output = Dense(output_neurons,\n",
        "               activation=None,\n",
        "               kernel_initializer=initializer)(x)\n",
        "model=ODE_2nd(input,output)\n",
        "\n",
        "# Definition of the metrics, optimizer and loss\n",
        "loss= tf.keras.losses.MeanSquaredError()\n",
        "metrics=tf.keras.metrics.MeanSquaredError()\n",
        "optimizer= Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "          optimizer=optimizer,\n",
        "          metrics=[metrics])\n",
        "model.summary()\n",
        "\n",
        "# Just use `fit` as usual\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                            patience=1000,\n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "history=model.fit(x_train, y_train,batch_size=1, epochs=epochs,\n",
        "                  callbacks=callback)"
      ],
      "metadata": {
        "id": "Bqg_6Bu0VZ_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evolution of losses during training"
      ],
      "metadata": {
        "id": "Dv4s61JnVjjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for loss and metris\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.plot(history.history['loss'],color='magenta',\n",
        "         label='Total losses ($L_D + L_B$)')\n",
        "plt.plot(history.history['mean_squared_error'],color='cyan',label='MSE')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yTfmSOVHV1Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution and its derivatives (to compare the numerical solution with RK4 and PINN)"
      ],
      "metadata": {
        "id": "5xUN7g-PV3tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the PINN at different points not included in the training set\n",
        "n = 500\n",
        "x=np.linspace(0,8,n)\n",
        "y_exact=tf.cos(-x)\n",
        "y_NN=model.predict(x)\n",
        "\n",
        "# The gradients (y'(x) and y''(x)) from the model\n",
        "x_tf = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "with tf.GradientTape(persistent=True) as t:\n",
        "  t.watch(x_tf)\n",
        "  with tf.GradientTape(persistent=True) as t2:\n",
        "        t2.watch(x_tf)\n",
        "        y = model(x_tf)\n",
        "  dy_dx_NN = t2.gradient(y, x_tf)\n",
        "d2y_dx2_NN = t.gradient(dy_dx_NN, x_tf)\n",
        "\n",
        "# Plot the results\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.plot(x, y_exact, color=\"black\",linestyle='solid',\n",
        "                     linewidth=2.5,label=\"$y(x)$ analytical\")\n",
        "plt.plot(x, y_NN, color=\"red\",linestyle='dashed',\n",
        "                  linewidth=2.5, label=\"$y_{NN}(x)$\")\n",
        "plt.plot(x, dy_dx_NN, color=\"blue\",linestyle='-.',\n",
        "                  linewidth=3.0, label=\"$y'_{NN}(x)$\")\n",
        "plt.plot(x, d2y_dx2_NN, color=\"green\", linestyle='dotted',\n",
        "                  linewidth=3.0, label=\"$y''_{NN}(x)$\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lh9EI_0cV-iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: $2^{nd}$ order nonlinear ODE (Korteweg-de Vries equation)\n",
        "\n",
        "$y''(x)-y(x)+3y^2(x)=0 \\hspace{0.3cm} with \\hspace{0.3cm} x_{min} < x < x_{max}$,\n",
        "            \n",
        "with initial conditions (effectively a boundary conditions) $y(0)=-\\frac{1}{2}$ and $y'(0)=0$.\n",
        "\n",
        "*The analytical solution (to be compared with):  $y(x)=-\\frac{1}{2} sech ^2(\\frac{x}{2})$*"
      ],
      "metadata": {
        "id": "hqzFwMx7WDSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import relevant libraries"
      ],
      "metadata": {
        "id": "INJKJ1TDXOKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow Keras and rest of the packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input,Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nF7zB_vVXU5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and construct the PINN"
      ],
      "metadata": {
        "id": "hhpLCpqmXcCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ODE_2nd(tf.keras.Model):\n",
        "    def train_step(self, data):\n",
        "        # Training points and the analytical\n",
        "        # (exact) solution at this points\n",
        "        x, y_exact = data\n",
        "        #Change initial conditions for the PINN\n",
        "        x0=tf.constant([0.0], dtype=tf.float32)\n",
        "        y0_exact=tf.constant([-0.5], dtype=tf.float32)\n",
        "        dy_dx0_exact=tf.constant([0.0], dtype=tf.float32)\n",
        "        C=tf.constant([1.0], dtype=tf.float32)\n",
        "        # Calculate the gradients and update weights and bias\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x)\n",
        "            tape.watch(y_exact)\n",
        "            tape.watch(x0)\n",
        "            tape.watch(y0_exact)\n",
        "            tape.watch(dy_dx0_exact)\n",
        "            # Calculate the gradients dy2/dx2, dy/dx\n",
        "            with tf.GradientTape() as tape0:\n",
        "                    tape0.watch(x0)\n",
        "                    y0_NN = self(x0,training=False)\n",
        "                    tape0.watch(y0_NN)\n",
        "            dy_dx0_NN = tape0.gradient(y0_NN, x0)\n",
        "            with tf.GradientTape() as tape1:\n",
        "                tape1.watch(x)\n",
        "                with tf.GradientTape() as tape2:\n",
        "                    tape2.watch(x)\n",
        "                    y_NN = self(x,training=False)\n",
        "                    tape2.watch(y_NN)\n",
        "                dy_dx_NN = tape2.gradient(y_NN, x)\n",
        "                tape1.watch(y_NN)\n",
        "                tape1.watch(dy_dx_NN)\n",
        "            d2y_dx2_NN = tape1.gradient(dy_dx_NN, x)\n",
        "            tape.watch(y_NN)\n",
        "            tape.watch(dy_dx_NN)\n",
        "            tape.watch(d2y_dx2_NN)\n",
        "\n",
        "            #Loss= ODE+ boundary/initial conditions\n",
        "            y0_exact=tf.reshape(y0_exact,shape=y_NN[0].shape)\n",
        "            dy_dx0_exact=tf.reshape(dy_dx0_exact,shape=dy_dx0_NN.shape)\n",
        "            C=tf.reshape(C,shape=d2y_dx2_NN.shape)\n",
        "\n",
        "            loss= self.compiled_loss(y0_NN,y0_exact)\\\n",
        "                  +self.compiled_loss(dy_dx0_NN,dy_dx0_exact)\\\n",
        "                  +self.compiled_loss(d2y_dx2_NN,C*y_NN+3.0*y_NN**2)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "        self.compiled_metrics.update_state(y_exact, y_NN)\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "TWCOEQuMXmxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the PINN and tune it\n",
        "\n",
        "Here you are invited to \"play around\" with the $hyperparameters$ that characterize dhe PINN. Try several combinations and look for sets of values that yield a smaller Loss Function"
      ],
      "metadata": {
        "id": "_NYjeODmXuIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = 11\n",
        "xmin = -5\n",
        "xmax = 5\n",
        "\n",
        "# Definition of the function domain\n",
        "x_train=np.linspace(xmin,xmax,n_train)\n",
        "\n",
        "# The solution y(x) for training and validation datasets\n",
        "x0=0.0\n",
        "y_train=-0.5*1.0*(1.0/np.cosh(0.5*np.sqrt(1.0)*(x_train-x0)))**2\n",
        "\n",
        "# Input and output neurons (from the data)\n",
        "input_neurons  = 1\n",
        "output_neurons = 1\n",
        "\n",
        "# Hiperparameters\n",
        "epochs = 2000\n",
        "\n",
        "# Definition of the model\n",
        "activation='tanh'\n",
        "input=Input(shape=(input_neurons,))\n",
        "x=Dense(50, activation=activation)(input)\n",
        "x=Dense(50, activation=activation)(x)\n",
        "x=Dense(50, activation=activation)(x)\n",
        "output = Dense(output_neurons,activation=None)(x)\n",
        "model=ODE_2nd(input,output)\n",
        "\n",
        "# Definition of the metrics, optimizer and loss\n",
        "loss=tf.keras.losses.MeanSquaredError()\n",
        "metrics=tf.keras.metrics.MeanSquaredError()\n",
        "optimizer= Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "          optimizer=optimizer,\n",
        "          metrics=[metrics])\n",
        "model.summary()\n",
        "\n",
        "# Just use `fit` as usual\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                            patience=1000,\n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "history=model.fit(x_train, y_train, batch_size=1, epochs=epochs,\n",
        "                  callbacks=callback)"
      ],
      "metadata": {
        "id": "qjbZHmRmX3UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evolution of losses during training"
      ],
      "metadata": {
        "id": "G8N9ChmrX8xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for loss and metris\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.plot(history.history['loss'],color='magenta',label='Total losses ($L_D + L_B$)')\n",
        "plt.plot(history.history['mean_squared_error'],color='cyan',label='MSE')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3LryPaO8YFgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution and its derivatives (to compare the numerical solution with RK4 and PINN)"
      ],
      "metadata": {
        "id": "G3JKRB6nYKGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the PINN at different points not included in the training set\n",
        "n = 500\n",
        "x=np.linspace(-6,6,n)\n",
        "x0=0.0\n",
        "y_exact=-0.5*1.0*(1.0/np.cosh(0.5*np.sqrt(1.0)*(x-x0)))**2\n",
        "y_NN=model.predict(x)\n",
        "\n",
        "# The gradients (y'(x) and y''(x)) from the model\n",
        "x_tf = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "with tf.GradientTape(persistent=True) as t:\n",
        "  t.watch(x_tf)\n",
        "  with tf.GradientTape(persistent=True) as t2:\n",
        "        t2.watch(x_tf)\n",
        "        y = model(x_tf)\n",
        "  dy_dx_NN = t2.gradient(y, x_tf)\n",
        "d2y_dx2_NN = t.gradient(dy_dx_NN, x_tf)\n",
        "\n",
        "# Plot the results\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.plot(x, y_exact,color=\"black\",linestyle='solid',\n",
        "                     linewidth=2.5,label=\"$y(x)$ analytical\")\n",
        "plt.plot(x, y_NN, color=\"red\",linestyle='dashed',\n",
        "                     linewidth=2.5, label=\"$y_{NN}(x)$\")\n",
        "plt.plot(x, dy_dx_NN,color=\"blue\",linestyle='-.',\n",
        "                     linewidth=3.0, label=\"$y'_{NN}(x)$\")\n",
        "plt.plot(x, d2y_dx2_NN,color=\"green\", linestyle='dotted',\n",
        "                     linewidth=3.0, label=\"$y''_{NN}(x)$\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "anqUBHIUYavT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework:\n",
        "\n",
        "Now that you succesfully finished the execises, try build your own PINN, tune it and numerically solve an ODE.\n",
        "\n",
        "#### Exercise 1: $1^{st}$ order ODE (body falling with air drag)\n",
        "\n",
        "$y'(x)+y(x)-1=0 \\hspace{0.3cm} with \\hspace{0.3cm} 0 < x < x_{max}$, with initial conditions (effectively a boundary conditions) $y(0)=0$. *The analytical solution (to be compared with):  $y(x)=1 - e^{-x}$*\n",
        "\n",
        "#### Exercise 2: $1^{st}$ order ODE (second order rate law)\n",
        "\n",
        "$y'(x)+y^2(x)=0 \\hspace{0.3cm} with \\hspace{0.3cm} 1 < x < x_{max}$, with initial conditions (effectively a boundary conditions) $y(0)=1$ and $y'(0)=0$. *The analytical solution (to be compared with):  $y(x)=\\frac{1}{x}$*\n",
        "\n",
        "#### Exercise 3: $2^{nd}$ order nonlinear ODE\n",
        "\n",
        "$y''(x)+x+x^2=0 \\hspace{0.3cm} with \\hspace{0.3cm} x_{min} < x < x_{max}$, with initial conditions (effectively a boundary conditions) $y(0)=0$ and $y'(0)=\\frac{1}{12}$. *The analytical solution (to be compared with):  $y(x)= \\frac{x^4}{12} - \\frac{x^3}{6} + \\frac{x}{12}$*\n",
        "\n",
        "\n",
        "Good luck!"
      ],
      "metadata": {
        "id": "5qquGxOYYcEb"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}